{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import itertools\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, SGDRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/cleaned_ex2.csv')\n",
    "df.fillna(360, inplace=True)\n",
    "Loan_Id = df['Loan_ID']\n",
    "df['LoanAmount'] = df['LoanAmount'].map(int)\n",
    "data_preprocess = df[['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'ApplicantIncome',\n",
    "       'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status']]\n",
    "# df.drop([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_preprocess.reset_index(inplace=True)\n",
    "data_preprocess[\"Married\"] = data_preprocess[\"Married\"].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "data_preprocess[\"Education\"] = data_preprocess[\"Education\"].apply(lambda x: 1 if x == 'Graduate' else 0)\n",
    "data_preprocess[\"Self_Employed\"]  = data_preprocess[\"Self_Employed\"].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "data_preprocess[\"Gender\"] = data_preprocess[\"Gender\"].apply(lambda x: 1 if x == 'Male' else 0)\n",
    "data_preprocess[\"Dependents\"] = data_preprocess[\"Dependents\"].apply(lambda x: 3 if x == '3+' else int(x))\n",
    "trn_data = pd.get_dummies(data_preprocess.loc[:,:'Property_Area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data.head()\n",
    "# df.head()\n",
    "index_X = df.Loan_Status[df.Loan_Status == 'X'].index\n",
    "index_YN = df.Loan_Status[df.Loan_Status != 'X'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['Gender', 'Married',\n",
    "#        'Dependents', 'Education', 'Self_Employed', 'ApplicantIncome',\n",
    "#        'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History',\n",
    "#        'Property_Area']]\n",
    "\n",
    "# loan_term_df = df[['Loan_Amount_Term', 'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Credit_History']]\n",
    "\n",
    "\n",
    "# loan_term_df = df[['Loan_Amount_Term', 'ApplicantIncome', 'LoanAmount', 'Credit_History']]\n",
    "\n",
    "# data_preprocess[\"Loan_Status\"].values\n",
    "\n",
    "# data_preprocess.info()\n",
    "\n",
    "taget_Var = df[\"Loan_Status\"].iloc[index_YN].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(trn_data.iloc[index_YN].values, df[\"Loan_Status\"].iloc[index_YN],  test_size=0.3, random_state= 24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loan_term_df['Self_Employed'] = loan_term_df['Self_Employed'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# df.columns\n",
    "# df[\"Loan_Status\"].iloc[index_YN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_trn = loan_term_df[loan_term_df[\"Loan_Amount_Term\"].notnull()]\n",
    "\n",
    "# data_trn.info()\n",
    "# y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8391608391608392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7621621621621621"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(trn_data.loc[:,\"ApplicantIncome\":].values, data_trn[\"Loan_Amount_Term\"].values,  test_size=0.01, random_state= 24)\n",
    "xgbRer = XGBClassifier(learning_rate=0.02, n_estimators=300, random_state=42, booster=\"gbtree\", n_jobs=1000)\n",
    "xgbRer.fit(X_train, y_train)\n",
    "print(xgbRer.score(X_train, y_train))\n",
    "y_pred = xgbRer.predict(X_test)\n",
    "xgbRer.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 21  30]\n",
      " [ 14 120]]\n",
      "\n",
      "accuracy  score:  0.7621621621621621\n",
      "classification error:  0.23783783783783785\n",
      "precision: 0.8\n",
      "recall or sensitivity 0.8955223880597015\n",
      "specificity 0.4117647058823529\n"
     ]
    }
   ],
   "source": [
    "# save confusion matrix and slice into four pieces\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(confusion)\n",
    "\n",
    "TP = confusion[1,1]\n",
    "TN = confusion[0,0]\n",
    "FP = confusion[0,1]\n",
    "FN = confusion[1,0]\n",
    "acc_score = (TP + TN)/float(TP+FP+FN+TN)\n",
    "classification_error = (FP + FN)/ float(TP + FP + FN + TN)\n",
    "precision = TP/float(TP + FP)\n",
    "sensitivity = recall = TP/float(TP+FN)\n",
    "specipecity = TN/float(TN + FP)\n",
    "\n",
    "print()\n",
    "print(\"accuracy  score: \", acc_score)\n",
    "print(\"classification error: \", classification_error)\n",
    "print(\"precision:\", precision)\n",
    "print(\"recall or sensitivity\", sensitivity)\n",
    "print(\"specificity\", specipecity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N' 'N' 'Y' 'N' 'Y' 'Y' 'Y' 'N' 'Y' 'Y']\n",
      "[[0.54667354 0.45332643]\n",
      " [0.68675256 0.31324744]\n",
      " [0.33230942 0.6676906 ]\n",
      " [0.7263261  0.27367392]\n",
      " [0.2244761  0.7755239 ]\n",
      " [0.15241706 0.84758294]\n",
      " [0.1560635  0.8439365 ]\n",
      " [0.66508436 0.33491564]\n",
      " [0.13046569 0.8695343 ]\n",
      " [0.0825808  0.9174192 ]]\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "print(xgbRer.predict(X_test)[0:10])\n",
    "\n",
    "#probability\n",
    "print(xgbRer.predict_proba(X_test)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'frequescy')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# store the predicted probabilities for class 1\n",
    "y_pred_prob = xgbRer.predict_proba(X_test)\n",
    "\n",
    "plt.hist(y_pred_prob)\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.title(\"Histogram of predicted probabilities\")\n",
    "plt.xlabel(\"Predicted probabilities of loan\")\n",
    "plt.ylabel(\"frequescy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict loans if the predicted probability is greater than 0.3\n",
    "from sklearn.preprocessing import binarize\n",
    "# it will return 1 for all values above 0.3 and 0 otherwise\n",
    "# results are 2D so we slice out the first column\n",
    "y_pred_class = binarize(y_pred_prob, 0.4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [185, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-617e8dd6fadc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\lokesh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lokesh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lokesh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 230\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [185, 2]"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7398373983739838"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrr = RandomForestClassifier(n_estimators=100, min_samples_split=2, n_jobs=1000)\n",
    "lrr.fit(X_train, y_train)\n",
    "print(lrr.score(X_train, y_train))\n",
    "lrr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([365.4 , 349.2 , 339.6 , 339.78, 340.68, 360.  , 355.2 , 298.2 ,\n",
       "       334.18, 355.2 , 357.6 , 345.24, 341.04, 343.44, 354.  , 343.8 ,\n",
       "       329.64, 340.2 , 342.12, 351.36])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['Loan_Amount_Term'].plot()\n",
    "# plt.show()\n",
    "lrr.predict(loan_term_df[loan_term_df[\"Loan_Amount_Term\"].isnull()].loc[:,\"ApplicantIncome\":].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loan_term_df[loan_term_df[\"Loan_Amount_Term\"] == 360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on XGBRegressor in module xgboost.sklearn object:\n",
      "\n",
      "class XGBRegressor(XGBModel, sklearn.base.RegressorMixin)\n",
      " |  Implementation of the scikit-learn API for XGBoost regression.\n",
      " |      Parameters\n",
      " |  ----------\n",
      " |  max_depth : int\n",
      " |      Maximum tree depth for base learners.\n",
      " |  learning_rate : float\n",
      " |      Boosting learning rate (xgb's \"eta\")\n",
      " |  n_estimators : int\n",
      " |      Number of boosted trees to fit.\n",
      " |  silent : boolean\n",
      " |      Whether to print messages while running boosting.\n",
      " |  objective : string or callable\n",
      " |      Specify the learning task and the corresponding learning objective or\n",
      " |      a custom objective function to be used (see note below).\n",
      " |  booster: string\n",
      " |      Specify which booster to use: gbtree, gblinear or dart.\n",
      " |  nthread : int\n",
      " |      Number of parallel threads used to run xgboost.  (Deprecated, please use n_jobs)\n",
      " |  n_jobs : int\n",
      " |      Number of parallel threads used to run xgboost.  (replaces nthread)\n",
      " |  gamma : float\n",
      " |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      " |  min_child_weight : int\n",
      " |      Minimum sum of instance weight(hessian) needed in a child.\n",
      " |  max_delta_step : int\n",
      " |      Maximum delta step we allow each tree's weight estimation to be.\n",
      " |  subsample : float\n",
      " |      Subsample ratio of the training instance.\n",
      " |  colsample_bytree : float\n",
      " |      Subsample ratio of columns when constructing each tree.\n",
      " |  colsample_bylevel : float\n",
      " |      Subsample ratio of columns for each split, in each level.\n",
      " |  reg_alpha : float (xgb's alpha)\n",
      " |      L1 regularization term on weights\n",
      " |  reg_lambda : float (xgb's lambda)\n",
      " |      L2 regularization term on weights\n",
      " |  scale_pos_weight : float\n",
      " |      Balancing of positive and negative weights.\n",
      " |  base_score:\n",
      " |      The initial prediction score of all instances, global bias.\n",
      " |  seed : int\n",
      " |      Random number seed.  (Deprecated, please use random_state)\n",
      " |  random_state : int\n",
      " |      Random number seed.  (replaces seed)\n",
      " |  missing : float, optional\n",
      " |      Value in the data which needs to be present as a missing value. If\n",
      " |      None, defaults to np.nan.\n",
      " |  **kwargs : dict, optional\n",
      " |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      " |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.md.\n",
      " |      Attempting to set a parameter via the constructor args and **kwargs dict simultaneously\n",
      " |      will result in a TypeError.\n",
      " |      Note:\n",
      " |          **kwargs is unsupported by Sklearn.  We do not guarantee that parameters passed via\n",
      " |          this argument will interact properly with Sklearn.\n",
      " |  \n",
      " |  Note\n",
      " |  ----\n",
      " |  A custom objective function can be provided for the ``objective``\n",
      " |  parameter. In this case, it should have the signature\n",
      " |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |  y_true: array_like of shape [n_samples]\n",
      " |      The target values\n",
      " |  y_pred: array_like of shape [n_samples]\n",
      " |      The predicted values\n",
      " |  \n",
      " |  grad: array_like of shape [n_samples]\n",
      " |      The value of the gradient for each sample point.\n",
      " |  hess: array_like of shape [n_samples]\n",
      " |      The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBRegressor\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  apply(self, X, ntree_limit=0)\n",
      " |      Return the predicted leaf every tree for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  evals_result(self)\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If eval_set is passed to the `fit` function, you can call evals_result() to\n",
      " |      get evaluation results for all passed eval_sets. When eval_metric is also\n",
      " |      passed to the `fit` function, the evals_result will contain the eval_metrics\n",
      " |      passed to the `fit` function\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result : dictionary\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      " |      \n",
      " |      clf = xgb.XGBModel(**param_dist)\n",
      " |      \n",
      " |      clf.fit(X_train, y_train,\n",
      " |              eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      " |              eval_metric='logloss',\n",
      " |              verbose=True)\n",
      " |      \n",
      " |      evals_result = clf.evals_result()\n",
      " |      \n",
      " |      The variable evals_result will contain:\n",
      " |      {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |       'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None)\n",
      " |      Fit the gradient boosting model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix\n",
      " |      y : array_like\n",
      " |          Labels\n",
      " |      sample_weight : array_like\n",
      " |          instance weights\n",
      " |      eval_set : list, optional\n",
      " |          A list of (X, y) tuple pairs to use as a validation set for\n",
      " |          early-stopping\n",
      " |      sample_weight_eval_set : list, optional\n",
      " |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      " |          instance weights on the i-th validation set.\n",
      " |      eval_metric : str, callable, optional\n",
      " |          If a str, should be a built-in evaluation metric to use. See\n",
      " |          doc/parameter.md. If callable, a custom evaluation metric. The call\n",
      " |          signature is func(y_predicted, y_true) where y_true will be a\n",
      " |          DMatrix object such that you may need to call the get_label\n",
      " |          method. It must return a str, value pair where the str is a name\n",
      " |          for the evaluation and value is the value of the evaluation\n",
      " |          function. This objective is always minimized.\n",
      " |      early_stopping_rounds : int\n",
      " |          Activates early stopping. Validation error needs to decrease at\n",
      " |          least every <early_stopping_rounds> round(s) to continue training.\n",
      " |          Requires at least one item in evals.  If there's more than one,\n",
      " |          will use the last. Returns the model from the last iteration\n",
      " |          (not the best one). If early stopping occurs, the model will\n",
      " |          have three additional fields: bst.best_score, bst.best_iteration\n",
      " |          and bst.best_ntree_limit.\n",
      " |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      " |          and/or num_class appears in the parameters)\n",
      " |      verbose : bool\n",
      " |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      " |          metric measured on the validation set to stderr.\n",
      " |      xgb_model : str\n",
      " |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      " |          loaded before training (allows training continuation).\n",
      " |  \n",
      " |  get_booster(self)\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  get_params(self, deep=False)\n",
      " |      Get parameters.\n",
      " |  \n",
      " |  get_xgb_params(self)\n",
      " |      Get xgboost type parameters.\n",
      " |  \n",
      " |  load_model(self, fname)\n",
      " |      Load the model from a file.\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or a memory buffer\n",
      " |          Input file name or memory buffer(see also save_raw)\n",
      " |  \n",
      " |  predict(self, data, output_margin=False, ntree_limit=None)\n",
      " |  \n",
      " |  save_model(self, fname)\n",
      " |      Save the model to a file.\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Output file name\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from XGBModel:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgbRer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RegressorMixin.score of RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1000,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lrr.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_preprocesss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      128\n",
       "1       66\n",
       "2      120\n",
       "3      141\n",
       "4      267\n",
       "5       95\n",
       "6      158\n",
       "7      168\n",
       "8      349\n",
       "9       70\n",
       "10     200\n",
       "11     114\n",
       "12      17\n",
       "13     125\n",
       "14     100\n",
       "15      76\n",
       "16     133\n",
       "17     104\n",
       "18     315\n",
       "19     116\n",
       "20     112\n",
       "21     191\n",
       "22     122\n",
       "23     110\n",
       "24      35\n",
       "25      74\n",
       "26     106\n",
       "27     114\n",
       "28     320\n",
       "29     100\n",
       "      ... \n",
       "951    125\n",
       "952     40\n",
       "953    122\n",
       "954    134\n",
       "955    101\n",
       "956    130\n",
       "957     98\n",
       "958    257\n",
       "959    156\n",
       "960    117\n",
       "961    128\n",
       "962     65\n",
       "963    401\n",
       "964     54\n",
       "965    118\n",
       "966     59\n",
       "967    141\n",
       "968    216\n",
       "969     88\n",
       "970    119\n",
       "971    137\n",
       "972    170\n",
       "973    126\n",
       "974    125\n",
       "975    159\n",
       "976    108\n",
       "977    191\n",
       "978    197\n",
       "979    316\n",
       "980    112\n",
       "Name: LoanAmount, Length: 981, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_data['LoanAmount'].map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
